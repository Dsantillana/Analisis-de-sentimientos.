{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importar librerias necesarias#\n",
    "from textblob import TextBlob\n",
    "import sys\n",
    "import tweepy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import nltk\n",
    "import pycountry\n",
    "import re\n",
    "import string\n",
    "import csv\n",
    "import json\n",
    "from tweepy import Stream\n",
    "from tweepy.streaming import StreamListener\n",
    "from tweepy import OAuthHandler\n",
    "\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from PIL import Image\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from langdetect import detect\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#TOKENS DE ACCESO#\n",
    "CONSUMER_TOKEN = \"gzNjoVdsjT3EZPet3cbhRnA2Q\"\n",
    "CONSUMER_SECRET = \"LLZmHMfiK4hmzUjlXaAw4QseEGw4YfMOvz2jVRpM6DAIqofc2V\"\n",
    "ACCESS_TOKEN = \"389120518-j1VFKmSuI92XqYIHAkNc5wKuM85j4OTvfvTg7mY4\"\n",
    "ACCESS_TOKEN_SECRET = \"nS1Qo3nH6VPUKBMHn8SbcOV8HPWDJdgHCvrlLaQREHhOz\"\n",
    "\n",
    "#AUTENTIFICACION#\n",
    "auth = tweepy.OAuthHandler(CONSUMER_TOKEN, CONSUMER_SECRET)\n",
    "auth.set_access_token(ACCESS_TOKEN, ACCESS_TOKEN_SECRET)\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "#Analisis de sentimientos inicial#\n",
    "def porcentage (part,whole):\n",
    "    return 100 * float(part)/float(whole)\n",
    "\n",
    "keyword = input (\"Introduce la palabra a buscar: \")\n",
    "noOfTweet = int(input(\"Introduce el nº de tweets a analizar: \"))\n",
    "\n",
    "tweetsP =tweepy.Cursor (api.search, q=keyword ,lang='en',until='2021-04-19').items(noOfTweet)\n",
    "positive = 0\n",
    "negative = 0\n",
    "neutral = 0\n",
    "polarity = 0\n",
    "tweet_list = []\n",
    "neutral_list = []\n",
    "negative_list = []\n",
    "positive_list = []\n",
    "date_list = []\n",
    "user_list = []\n",
    "location_list = []\n",
    "posdate_list = []\n",
    "posuser_list = []\n",
    "poslocation_list = []\n",
    "neudate_list = []\n",
    "neuuser_list = []\n",
    "neulocation_list = []\n",
    "negdate_list = []\n",
    "neguser_list = []\n",
    "neglocation_list = []\n",
    "for tweet in tweetsP:\n",
    "    date_list.append(tweet.created_at)\n",
    "    tweet_list.append(tweet.text)\n",
    "    user_list.append(tweet.user.screen_name)\n",
    "    location_list.append(tweet.user.location)\n",
    "    analysis = TextBlob(tweet.text)\n",
    "    score = SentimentIntensityAnalyzer() .polarity_scores(tweet.text)\n",
    "    neg = score['neg']\n",
    "    neu = score['neu']\n",
    "    pos = score['pos']\n",
    "    comp = score['compound']\n",
    "    polarity += analysis.sentiment.polarity\n",
    "    \n",
    "    if neg > pos:\n",
    "        negdate_list.append(tweet.created_at)\n",
    "        negative_list.append(tweet.text)\n",
    "        neguser_list.append(tweet.user.screen_name)\n",
    "        neglocation_list.append(tweet.user.location)\n",
    "        negative += 1\n",
    "    elif pos > neg:\n",
    "        posdate_list.append(tweet.created_at)\n",
    "        positive_list.append(tweet.text)\n",
    "        posuser_list.append(tweet.user.screen_name)\n",
    "        poslocation_list.append(tweet.user.location)\n",
    "        positive += 1\n",
    "    elif pos == neg:\n",
    "        neudate_list.append(tweet.created_at)\n",
    "        neutral_list.append(tweet.text)\n",
    "        neuuser_list.append(tweet.user.screen_name)\n",
    "        neulocation_list.append(tweet.user.location)\n",
    "        neutral += 1\n",
    "positive = porcentage(positive, noOfTweet)\n",
    "negative = porcentage(negative, noOfTweet)\n",
    "neutral = porcentage(neutral, noOfTweet)\n",
    "polarity = porcentage(polarity, noOfTweet)\n",
    "positive = format(positive,'.1f')\n",
    "negative = format(negative,'.1f')\n",
    "neutral = format(neutral,'.1f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nº de Tweets recolectados (Total, Positivo, Negativo, Neutro)#\n",
    "df = pd.DataFrame ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construcción del DataFrame#\n",
    "df['TweetAt'] = date_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ScreenName'] = user_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Location'] = location_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['OriginalTweet'] = tweet_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfpos = pd.DataFrame ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfpos['TweetAT'] = posdate_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfpos['ScreenName'] = posuser_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfpos['Location'] = poslocation_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfpos['OriginalTweet'] = positive_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfneu = pd.DataFrame ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfneu['TweetAT'] = neudate_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfneu['ScreenName'] = neuuser_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfneu['Location'] = neulocation_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfneu['OriginalTweet'] = neutral_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfneg = pd.DataFrame ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfneg['TweetAT'] = negdate_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfneg['ScreenName'] = neguser_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfneg['Location'] = neglocation_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfneg['OriginalTweet'] = negative_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Agregar a nuestra tabla principal el sentimiento de cada tweet#\n",
    "for index, row in df['OriginalTweet'].iteritems():\n",
    "    score = SentimentIntensityAnalyzer().polarity_scores(row)\n",
    "    neg = score['neg']\n",
    "    neu = score['neu']\n",
    "    pos = score['pos']\n",
    "    comp = score['compound']\n",
    "    if neg > pos:\n",
    "        df.loc[index, 'sentiment'] = \"negative\"\n",
    "    elif pos > neg:\n",
    "        df.loc[index, 'sentiment'] = \"positive\"\n",
    "    else:\n",
    "        df.loc[index, 'sentiment'] = \"neutral\"\n",
    "    df.loc[index, 'neg'] = neg\n",
    "    df.loc[index, 'neu'] = neu\n",
    "    df.loc[index, 'pos'] = pos\n",
    "    df.loc[index, 'compound'] = comp\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocesamiento de los datos#\n",
    "#Eliminacion de usuarios (@)#\n",
    "\n",
    "def remove_pattern(input_txt, pattern):\n",
    "    r = re.findall(pattern, input_txt)\n",
    "    for i in r:\n",
    "        input_txt = re.sub(i,'',input_txt)\n",
    "    return input_txt\n",
    "\n",
    "# create new column with removed @user\n",
    "df['Tweet'] = np.vectorize(remove_pattern)(df['OriginalTweet'], '@[\\w]*')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminacion de enlaces#\n",
    "\n",
    "import re\n",
    "df['Tweet'] = df['Tweet'].apply(lambda x: re.split('https:\\/\\/.*', str(x))[0])\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminacion de caracteres especiales, signos de puntuación  y nº#\n",
    "\n",
    "# remove special characters, numbers, punctuations\n",
    "df['Tweet'] = df['Tweet'].str.replace('[^a-zA-Z#]+',' ')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tokenizacion#\n",
    "df['tokenizado'] = df['Tweet'].apply(lambda x: x.split())\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guardar el DF para trabajar posteriormente con el#\n",
    "df.to_csv(\"C:\\Tweets\\Johnson1904puro.csv\", header=True, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importar librerias necesarias#\n",
    "from textblob import TextBlob\n",
    "import sys\n",
    "import tweepy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import nltk\n",
    "import pycountry\n",
    "import re\n",
    "import string\n",
    "import csv\n",
    "import json\n",
    "from tweepy import Stream\n",
    "from tweepy.streaming import StreamListener\n",
    "from tweepy import OAuthHandler\n",
    "import glob\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from PIL import Image\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from langdetect import detect\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importación de los distintos archivos de recolección en uno solo#\n",
    "path = r'C:\\AllTweets' # use your path\n",
    "all_files = glob.glob(path + \"/*.csv\")\n",
    "\n",
    "li = []\n",
    "\n",
    "for filename in all_files:\n",
    "    df = pd.read_csv(filename, index_col=None, header=0, sep='\\t' )\n",
    "    li.append(df)\n",
    "\n",
    "frame = pd.concat(li, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Información sobre nuestro dataframe#\n",
    "frame.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dividir la columna \"TweetAt\" para separarlo en fecha y hora#\n",
    "new = frame[\"TweetAt\"].str.split(\" \", n = 1, expand = True)\n",
    "frame[\"Date\"]= new[0]\n",
    "frame[\"Hour\"]= new[1]\n",
    "frame.drop(columns =[\"TweetAt\"], inplace = True)\n",
    "frame.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gráfico fechas#\n",
    "fechas = frame['Date'].value_counts()\n",
    "graf = sns.catplot(\"Date\", data=frame, kind=\"count\", height=15)\n",
    "graf.fig.suptitle('Frecuencia de fechas')\n",
    "graf.savefig('C:\\AllTweets\\Todasfechas.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gráfico sentimientos#\n",
    "sentimiento = frame['sentiment'].value_counts()\n",
    "graf = sns.catplot(\"sentiment\", data=frame, kind=\"count\", height=8)\n",
    "graf.fig.suptitle('Frecuencia de sentimientos')\n",
    "graf.savefig('C:\\AllTweets\\Todassentimiento.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminación de columnas innecesarias#\n",
    "frame = frame.drop(columns=['Unnamed: 0', 'neg', 'neu', 'pos', 'compound', 'Tweet', 'tokenizado','Hour'])\n",
    "frame.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PREPROCESAMIENTO DE LOS DATOS#\n",
    "#Eliminación de los usuarios mencionados en los tweet#\n",
    "def remove_pattern(input_txt, pattern):\n",
    "    r = re.findall(pattern, input_txt)\n",
    "    for i in r:\n",
    "        input_txt = re.sub(i,'',input_txt)\n",
    "    return input_txt\n",
    "\n",
    "frame['Tweet'] = np.vectorize(remove_pattern)(frame['OriginalTweet'], '@[\\w]*')\n",
    "frame.head(2).to_excel('C:\\AllTweets\\PruebaP.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminación de las URL#\n",
    "import re\n",
    "frame['Tweet'] = frame['Tweet'].apply(lambda x: re.split('https:\\/\\/.*', str(x))[0])\n",
    "frame.head(2).to_excel('C:\\AllTweets\\EliminaciónURL.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminacion de signos de puntuación, nº y caracteres especiales#\n",
    "frame['Tweet'] = frame['Tweet'].str.replace('[^a-zA-Z#]+',' ')\n",
    "frame.head(2).to_excel('C:\\AllTweets\\Eliminaciónpuntuacion.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminación de \"Short Words\"#\n",
    "frame['Tweet'] = frame['Tweet'].apply(lambda x: ' '.join([w for w in x.split() if len(w) > 2]))\n",
    "frame.head(10).to_excel('C:\\AllTweets\\Eliminaciónshortwords.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizacion#\n",
    "token_tweet = frame['Tweet'].apply(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Derivacion#\n",
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "token_tweet = token_tweet.apply(lambda x: [stemmer.stem(i) for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sustituir la antiguo columna de Tweet por una nueva tokenizada y derivada#\n",
    "frame['Tweet'] = token_tweet\n",
    "frame.head(10).to_excel('C:\\AllTweets\\Stemming.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminación de las lineas que contegan el mismo tweet#\n",
    "frame.drop_duplicates(subset=['Tweet'],inplace=True)\n",
    "frame.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wordcloud para todos los tweets#\n",
    "all_words = ' '.join([text for text in frame['Tweet']])\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.savefig('C:\\AllTweets\\Cloudall.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WordCloud para tweets positivos#\n",
    "positive_words = ' '.join([text for text in frame['Tweet'][frame['sentiment'] == 'positive']])\n",
    "wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(positive_words)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.savefig('C:\\AllTweets\\Cloudpositive.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WordCloud para tweets negativos#\n",
    "negative_words = ' '.join([text for text in frame['Tweet'][frame['sentiment'] == 'negative']])\n",
    "wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(negative_words)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.savefig('C:\\AllTweets\\Cloudnegative.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WordCloud para tweets neutrales#\n",
    "neutral_words = ' '.join([text for text in frame['Tweet'][frame['sentiment'] == 'neutral']])\n",
    "wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(neutral_words)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.savefig('C:\\AllTweets\\Cloudneutral.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recolección de hastags#\n",
    "def hashtag_extract(x):\n",
    "    hashtags = []\n",
    "    for i in x:\n",
    "        ht = re.findall(r'#(\\w+)', i)\n",
    "        hashtags.append(ht)\n",
    "    return hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HT_Positive = hashtag_extract(frame['OriginalTweet'][frame['sentiment'] == 'positive'])\n",
    "HT_Neutral = hashtag_extract(frame['OriginalTweet'][frame['sentiment'] == 'neutral'])\n",
    "HT_Negative = hashtag_extract(frame['OriginalTweet'][frame['sentiment'] == 'negative'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HT_Positive = sum(HT_Positive, [])\n",
    "HT_Neutral = sum(HT_Neutral, [])\n",
    "HT_Negative = sum(HT_Negative,[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gráfico para hastafs de tweets positivos#\n",
    "a = nltk.FreqDist(HT_Positive)\n",
    "d = pd.DataFrame({'Hashtag': list(a.keys()),\n",
    "                  'Count' : list(a.values())})\n",
    "\n",
    "d = d.nlargest(columns = 'Count', n = 10)\n",
    "\n",
    "plt.figure(figsize = (16,5))\n",
    "ax = sns.barplot(data =d, x = 'Hashtag', y = 'Count')\n",
    "plt.savefig('C:\\AllTweets\\Hastagpositivo.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graáfico para hashtags de tweets neutrales#\n",
    "a = nltk.FreqDist(HT_Neutral)\n",
    "d = pd.DataFrame({'Hashtag': list(a.keys()),\n",
    "                  'Count' : list(a.values())})\n",
    "\n",
    "d = d.nlargest(columns = 'Count', n = 10)\n",
    "\n",
    "plt.figure(figsize = (16,5))\n",
    "ax = sns.barplot(data =d, x = 'Hashtag', y = 'Count')\n",
    "plt.savefig('C:\\AllTweets\\Hastagneutral.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graáfico para hashtags de tweets neutrales#\n",
    "a = nltk.FreqDist(HT_Negative)\n",
    "d = pd.DataFrame({'Hashtag': list(a.keys()),\n",
    "                  'Count' : list(a.values())})\n",
    "\n",
    "d = d.nlargest(columns = 'Count', n = 10)\n",
    "\n",
    "plt.figure(figsize = (16,5))\n",
    "ax = sns.barplot(data =d, x = 'Hashtag', y = 'Count')\n",
    "plt.savefig('C:\\AllTweets\\Hastagnegaivo.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dejar solo las columnas con las que vamos a trabjar a partir de ahora (Tweet depurado y sentimiento)#\n",
    "new_frame = frame[['Tweet', 'sentiment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminar mayusculas#\n",
    "\n",
    "new_frame[\"Tweet\"] = new_frame[\"Tweet\"].str.lower().str.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminacion de StopWords#\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "new_frame['Tweet'].apply(lambda x: [item for item in x if item not in stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unir los tokens en una frase#\n",
    "new_frame['Tweet'] = new_frame['Tweet'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comprobar ele tamaño de nuestro nuevo df#\n",
    "new_frame.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Detectar si hay algun valor vacio#\n",
    "\n",
    "new_frame.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Proceso de modelizacion#\n",
    "#Diviendo nuestro dataset en train y test#\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train,valid = train_test_split(new_frame,test_size = 0.2,random_state=42, stratify = new_frame.sentiment.values) #stratification means that the train_test_split method returns training and test subsets that have the same proportions of class labels as the input dataset.\n",
    "print(\"train shape : \", train.shape)\n",
    "print(\"valid shape : \", valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aplicando COUNTVECTORIZER#\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "stop = list(stopwords.words('english'))\n",
    "vectorizer = CountVectorizer(decode_error = 'replace',stop_words = stop)\n",
    "\n",
    "X_train = vectorizer.fit_transform(train.Tweet.values)\n",
    "X_valid = vectorizer.transform(valid.Tweet.values)\n",
    "\n",
    "y_train = train.sentiment.values\n",
    "y_valid = valid.sentiment.values\n",
    "\n",
    "print(\"X_train.shape : \", X_train.shape)\n",
    "print(\"X_train.shape : \", X_valid.shape)\n",
    "print(\"y_train.shape : \", y_train.shape)\n",
    "print(\"y_valid.shape : \", y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST TUNEADO DE PARAMETROS RF#\n",
    "# Ver que parametros podemos modificar de nuestro algoritmo#\n",
    "from pprint import pprint\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(random_state = 42)\n",
    "print('Parametros en uso')\n",
    "pprint(rf.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Crear parrilla de hyperparametros Random#\n",
    "#Nº de arboles en el RF#\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "#Nº nº de caracteristicas a tener en cuenta para cada division#\n",
    "max_features = ['sqrt']\n",
    "#Numero maximo de niveles en el arbol#\n",
    "max_depth = [int(x) for x in np.linspace(10,110, num = 11)]\n",
    "#Tamaño minimo de muestras para dividir un nodo#\n",
    "min_samples_split = [2, 5, 7]\n",
    "#Tamaño minimo de hoja#\n",
    "min_samples_leaf = [ 2, 4]\n",
    "#Metodo de seleccion de muestra para entrenar cada arbol#\n",
    "bootstrap = [True, False]\n",
    "#Criterio#\n",
    "criterion = ['gini', 'entropy']\n",
    "\n",
    "#Creando la parrilla una vez indicadas los diferentes tuneados#\n",
    "random_grid = {'n_estimators': n_estimators,'max_features': max_features, 'max_depth': max_depth, 'min_samples_split': min_samples_split,'min_samples_leaf': min_samples_leaf, 'bootstrap': bootstrap, 'criterion': criterion}\n",
    "\n",
    "pprint(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Search Trainning#\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "rf_clf = RandomForestClassifier()\n",
    "\n",
    "rf_random = RandomizedSearchCV(estimator = rf_clf, param_distributions = random_grid, n_iter = 100, cv = 5,verbose=2, random_state=42, n_jobs = -1)\n",
    "\n",
    "rf_random.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_random.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GRID SEARCH#\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "#Aplicando grid search#\n",
    "param_grid = {'bootstrap': [False], 'max_depth': [110], 'max_features': ['sqrt'], 'min_samples_leaf': [2, 3, 5], 'min_samples_split': [2, 5, 10], 'n_estimators': [200, 500, 700], 'criterion' : ['gini']}\n",
    "\n",
    "#Creando modelo base#\n",
    "rf_clf = RandomForestClassifier(random_state = 42)\n",
    "\n",
    "grid_search = GridSearchCV(estimator = rf_clf, param_grid = param_grid, cv = 5,verbose = 2, n_jobs = -1, return_train_score=True)\n",
    "\n",
    "grid_search.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mejores parametros de Grid Search#\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluacion del mejor modelo del Grid Search#\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error, make_scorer,classification_report,confusion_matrix,accuracy_score,roc_auc_score,roc_curve\n",
    "best_rf_clf = RandomForestClassifier(random_state = 42, max_depth = None, max_features = 110, min_samples_leaf = 1, min_samples_split = 2, n_estimators = 1000, criterion = 'gini', bootstrap = False)\n",
    "\n",
    "best_rf_clf.fit(X_train,y_train)\n",
    "\n",
    "best_rf_prediction = best_rf_clf.predict(X_valid)\n",
    "best_rf_accuracy = accuracy_score(y_valid,best_rf_prediction)\n",
    "print(\"Training accuracy Score    : \",best_rf_clf.score(X_train,y_train))\n",
    "print(\"Validation accuracy Score : \",best_rf_accuracy )\n",
    "print(classification_report(best_rf_prediction,y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prueba RF sin especificaciones#\n",
    "rf_clf = RandomForestClassifier(random_state = 42)\n",
    "\n",
    "rf_clf.fit(X_train,y_train)\n",
    "\n",
    "rf_prediction = rf_clf.predict(X_valid)\n",
    "rf_accuracy = accuracy_score(y_valid,rf_prediction)\n",
    "print(\"Training accuracy Score    : \",rf_clf.score(X_train,y_train))\n",
    "print(\"Validation accuracy Score : \",rf_accuracy )\n",
    "print(classification_report(rf_prediction,y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modelo Naive Bayesano multiclass#\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error, make_scorer,classification_report,confusion_matrix,accuracy_score,roc_auc_score,roc_curve\n",
    "naiveByes_clf = MultinomialNB()\n",
    "\n",
    "naiveByes_clf.fit(X_train,y_train)\n",
    "\n",
    "NB_prediction = naiveByes_clf.predict(X_valid)\n",
    "NB_accuracy = accuracy_score(y_valid,NB_prediction)\n",
    "print(\"training accuracy Score    : \",naiveByes_clf.score(X_train,y_train))\n",
    "print(\"Validation accuracy Score : \",NB_accuracy )\n",
    "print(classification_report(NB_prediction,y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(naiveByes_clf.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RandomSearch NB#\n",
    "alpha = [0, 1, 2]\n",
    "class_prior = [None]\n",
    "fit_prior = [True, False]\n",
    "\n",
    "nbr_grid = {'alpha': alpha, 'class_prior': class_prior, 'fit_prior': fit_prior}\n",
    "nb_random = RandomizedSearchCV(estimator = naiveByes_clf, param_distributions = nbr_grid, n_iter = 100, cv = 5,verbose=2, random_state=42, n_jobs = -1)\n",
    "\n",
    "nb_random.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error,mean_absolute_error, make_scorer,classification_report,confusion_matrix,accuracy_score,roc_auc_score,roc_curve\n",
    "best_NB = MultinomialNB( fit_prior = False, class_prior = None, alpha = 0)\n",
    "\n",
    "best_NB.fit(X_train,y_train)\n",
    "\n",
    "best_NB_prediction = best_NB.predict(X_valid)\n",
    "best_NB_accuracy = accuracy_score(y_valid,best_NB_prediction)\n",
    "print(\"Training accuracy Score    : \",best_NB.score(X_train,y_train))\n",
    "print(\"Validation accuracy Score : \",best_NB_accuracy )\n",
    "print(classification_report(best_NB_prediction,y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM#\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svc = SVC(gamma = 'scale')\n",
    "\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "svc_prediction = svc.predict(X_valid)\n",
    "svc_accuracy = accuracy_score(y_valid,svc_prediction)\n",
    "print(\"Training accuracy Score    : \",svc.score(X_train,y_train))\n",
    "print(\"Validation accuracy Score : \",svc_accuracy )\n",
    "print(classification_report(svc_prediction,y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc.get_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RandomSearch SVM#\n",
    "C = [1, 5, 10, 15]\n",
    "class_weight = [None, 'balanced']\n",
    "decision_function_shape = ['ovo']\n",
    "kernel =['poly', 'rbf','sigmoid' ]\n",
    "gamma = ['scale', 'auto']\n",
    "\n",
    "svmr_grid = {'C': C, 'class_weight': class_weight, 'decision_function_shape': decision_function_shape, 'kernel': kernel, 'gamma': gamma}\n",
    "svm_random = RandomizedSearchCV(estimator = svc, param_distributions = svmr_grid, n_iter = 100, cv = 5,verbose=2, random_state=42, n_jobs = -1)\n",
    "\n",
    "svm_random.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aplicando grid search#\n",
    "param_grid_SVM = {'kernel': ['rbf'], 'gamma': ['scale'], 'decision_function_shape': ['ovo'], 'class_weight': [None], 'C': [2, 3, 4,5]}\n",
    "\n",
    "#Creando modelo base#\n",
    "SVM_clf = SVC(random_state = 42)\n",
    "\n",
    "grid_search_SVM = GridSearchCV(estimator = SVM_clf, param_grid = param_grid_SVM, cv = 5,verbose = 2, n_jobs = -1, return_train_score=True)\n",
    "\n",
    "grid_search_SVM.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_SVM.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluacion del mejor modelo del Grid Search SVM#\n",
    "\n",
    "best_SVM_clf = SVC(random_state = 42, kernel = 'rbf', gamma = 'scale', decision_function_shape = 'ovo', class_weight = None, C = 0.5)\n",
    "\n",
    "best_SVM_clf.fit(X_train,y_train)\n",
    "\n",
    "best_SVM_prediction = best_SVM_clf.predict(X_valid)\n",
    "best_SVM_accuracy = accuracy_score(y_valid,best_SVM_prediction)\n",
    "print(\"Training accuracy Score    : \",best_SVM_clf.score(X_train,y_train))\n",
    "print(\"Validation accuracy Score : \",best_SVM_accuracy )\n",
    "print(classification_report(best_SVM_prediction,y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regresión Logística#\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "logreg_prediction = logreg.predict(X_valid)\n",
    "logreg_accuracy = accuracy_score(y_valid,logreg_prediction)\n",
    "print(\"Training accuracy Score    : \",logreg.score(X_train,y_train))\n",
    "print(\"Validation accuracy Score : \",logreg_accuracy )\n",
    "print(classification_report(logreg_prediction,y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg.get_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RandomSearch Regresión logistica#\n",
    "C = [0, 1, 5, 10]\n",
    "class_weight = [None, 'balanced']\n",
    "solver = ['newton-cg', 'lbfgs', 'sag', 'saga']\n",
    "penalty =['l2', None ]\n",
    "warm_start = ['scale', 'auto']\n",
    "\n",
    "svmr_grid = {'C': C, 'class_weight': class_weight, 'decision_function_shape': decision_function_shape, 'kernel': kernel, 'gamma': gamma}\n",
    "svm_random = RandomizedSearchCV(estimator = svc, param_distributions = svmr_grid, n_iter = 100, cv = 5,verbose=2, random_state=42, n_jobs = -1)\n",
    "\n",
    "svm_random.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
